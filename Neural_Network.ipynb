{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# numpy implementation of neural nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    a = 1/(1 + np.exp(-z))\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    z = np.copy(Z)\n",
    "    np.maximum(z,0,z)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(Z,slope=0.001):\n",
    "    z = np.copy(Z)\n",
    "    np.maximum(z,slope*z,z)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_leaky(a,slope=0.001):\n",
    "    da = np.int64(a>0)*1 + np.int64(a<=0)*slope\n",
    "    return da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    a = np.exp(z)/np.sum(np.exp(z),axis=0)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_relu(a):\n",
    "    da = np.int64(a>0)*1\n",
    "    return da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_sig(a):\n",
    "    da = a * (1-a)\n",
    "    return da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    # layer_dims is a list of nodes in each layer. architechture of the network\n",
    "    # we are doing he-initialization\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    for l in range(1,L):\n",
    "        parameters[\"W\"+str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1]) * np.sqrt(2/layer_dims[l-1])\n",
    "        parameters[\"b\"+str(l)] = np.zeros((layer_dims[l],1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_activations(activations,L):\n",
    "    #activations: list of strings of activations\n",
    "    #L: no. of layers (excluding input)\n",
    "    acts = {}\n",
    "    if activations is None:\n",
    "        for l in range(1,L):\n",
    "            acts[\"act\"+str(l)] = \"relu\"\n",
    "        acts[\"act\"+str(L)] = \"sigmoid\"\n",
    "    else:\n",
    "        for l in range(1,L+1):\n",
    "            acts[\"act\"+str(l)] = activations[l-1]\n",
    "    return acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keep_probs(d):\n",
    "    if d is None:\n",
    "        return None\n",
    "    else :\n",
    "        d = np.array(d)\n",
    "        a = 1-d\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_z(A_prev,W,b):\n",
    "    # A_prev: activation from prev layer\n",
    "    # W,b: weights and bias of current layer\n",
    "    \n",
    "    z = np.dot(W,A_prev) + b\n",
    "    \n",
    "    assert(z.shape == (W.shape[0],A_prev.shape[1]))\n",
    "    cache = (A_prev,W,b)\n",
    "    \n",
    "    return z,cache #cache format returned for a layer: (A_prev,W,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_activation(A_prev,W,b,activation):\n",
    "    z,cache = find_z(A_prev,W,b)\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        a = sigmoid(z)\n",
    "    elif activation == \"relu\":\n",
    "        a = relu(z)\n",
    "    elif activation == \"softmax\":\n",
    "        a = softmax(z)\n",
    "    elif activation == \"leaky_relu\":\n",
    "        a = leaky_relu(z)\n",
    "    \n",
    "    assert (a.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (cache,a)\n",
    "    \n",
    "    return a,cache #cache format for a layer: ((A_prev,W,b),A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feed forward network\n",
    "def forward_propagation(X,parameters,acts=None):\n",
    "    L = len(parameters)//2\n",
    "    A = X\n",
    "    caches = []\n",
    "    \n",
    "    if acts is None:\n",
    "        acts = organize_activations(acts,L)\n",
    "    \n",
    "    for l in range(1,L):\n",
    "        A_prev = A\n",
    "        A,cache = forward_activation(A_prev,parameters[\"W\"+str(l)],parameters[\"b\"+str(l)],activation=acts[\"act\"+str(l)])\n",
    "        caches.append(cache)\n",
    "        \n",
    "    A_L,cache = forward_activation(A,parameters[\"W\"+str(L)],parameters[\"b\"+str(L)],activation=acts[\"act\"+str(L)])\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return A_L,caches #caches is for each layer(except input): ((A_prev,W,b),A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward prop with dropouts:\n",
    "def forward_activation_with_dropouts(A_prev,W,b,activation,keep_prob):\n",
    "    z,cache = find_z(A_prev,W,b)\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        a = sigmoid(z)\n",
    "    elif activation == \"relu\":\n",
    "        a = relu(z)\n",
    "    elif activation == \"softmax\":\n",
    "        a = softmax(z)\n",
    "    elif activation == \"leaky_relu\":\n",
    "        a = leaky_relu(z)\n",
    "    \n",
    "    #dropping out\n",
    "    D = np.random.rand(a.shape[0],a.shape[1])\n",
    "    D = D < keep_prob\n",
    "    a = a * D\n",
    "    a = a / keep_prob\n",
    "    \n",
    "    assert (a.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (cache,a,D)\n",
    "    \n",
    "    return a,cache #cache format for a layer: ((A_prev,W,b),A,D)\n",
    "\n",
    "def forward_propagation_with_dropouts(X,parameters,keep_probs,acts=None):\n",
    "    #keep_probs: list of keep_probs\n",
    "    L = len(parameters)//2\n",
    "    A = X\n",
    "    caches = []\n",
    "    \n",
    "    if acts is None:\n",
    "        acts = organize_activations(acts,L)\n",
    "    \n",
    "    for l in range(1,L):\n",
    "        A_prev = A\n",
    "        A,cache = forward_activation_with_dropouts(A_prev,parameters[\"W\"+str(l)],parameters[\"b\"+str(l)],activation=acts[\"act\"+str(l)],keep_prob=keep_probs[l-1])\n",
    "        caches.append(cache)\n",
    "        \n",
    "    A_L,cache = forward_activation_with_dropouts(A,parameters[\"W\"+str(L)],parameters[\"b\"+str(L)],activation=acts[\"act\"+str(L)],keep_prob=1)\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return A_L,caches # caches for each layer: ((A_prev,W,b),A,D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(h,y):\n",
    "    #L_sum = np.sum(y*np.log(h)) + np.sum((1-y)*(np.log(1-h)))\n",
    "    #even if one prediction is way off then cost shoots up to inf. you can use any of the two eqns in this code\n",
    "    L_sum = np.sum(np.log(h[y==1])) + np.sum(np.log(1-h[y==0]))\n",
    "    m = y.shape[1]\n",
    "    L = -(1/m) * L_sum\n",
    "\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_with_regularization(h,y,parameters,lambd):\n",
    "    m = y.shape[1]\n",
    "    L = len(parameters)//2\n",
    "    cross_entropy_cost = loss(h,y)\n",
    "    L2_regularization_cost = 0\n",
    "    for l in range(L):\n",
    "        L2_regularization_cost += np.sum(np.square(parameters[\"W\"+str(l+1)]))\n",
    "    L2_regularization_cost = lambd * L2_regularization_cost / (2*m)\n",
    "    cost = cross_entropy_cost + L2_regularization_cost\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_grads(dz,cache,lambd):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = (np.dot(dz,A_prev.T) / m) + (lambd * W)/m\n",
    "    db = np.sum(dz,axis=1,keepdims=True) / m\n",
    "    dA_prev = np.dot(W.T,dz)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backwards(dA,cache,activation,lambd,keep_prop=None):\n",
    "    if keep_prop is None:\n",
    "        c,A = cache\n",
    "    else:\n",
    "        c,A,D = cache\n",
    "        dA = dA * D\n",
    "        dA = dA / keep_prop\n",
    "        A = A * keep_prop # just to find accurate derivative for my defined functions. \n",
    "        #the above line won't matter in relu/leaky but better approximation for sigmoid\n",
    "        \n",
    "    if activation == \"relu\":\n",
    "        dz = dA * derivative_relu(A)\n",
    "    \n",
    "    elif activation == \"sigmoid\":\n",
    "        dz = dA * derivative_sig(A)\n",
    "        \n",
    "    elif activation == \"leaky_relu\":\n",
    "        dz = dA * derivative_leaky(A)\n",
    "     \n",
    "    dA_prev, dW, db = find_grads(dz, c,lambd)\n",
    "    return dA_prev,dW,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(A_L,Y,caches,lambd,acts=None,keep_probs=None):\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = A_L.shape[1]\n",
    "    Y = Y.reshape(A_L.shape)\n",
    "    \n",
    "    if not acts:\n",
    "        acts = organize_activations(acts,L)\n",
    "    \n",
    "    \n",
    "    cache = caches[-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = find_grads((A_L-Y),cache[0],lambd)\n",
    "    \n",
    "    if keep_probs is None:\n",
    "        for l in reversed(range(L-1)):\n",
    "            cache = caches[l]\n",
    "            grads[\"dA\" + str(l)], grads[\"dW\" + str(l+1)], grads[\"db\" + str(l+1)] = linear_backwards(grads[\"dA\"+str(l+1)],cache,activation=acts[\"act\"+str(l+1)],lambd=lambd)\n",
    "    \n",
    "    else:\n",
    "        for l in reversed(range(L-1)):\n",
    "            cache = caches[l]\n",
    "            grads[\"dA\" + str(l)], grads[\"dW\" + str(l+1)], grads[\"db\" + str(l+1)] = linear_backwards(grads[\"dA\"+str(l+1)],cache,activation=acts[\"act\"+str(l+1)],lambd=lambd,keep_prop=keep_probs[l])\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters,grads,learning_rate):\n",
    "    L = len(parameters)//2\n",
    "    \n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X,Y,batch_size,shuffle):\n",
    "    m = X.shape[1]\n",
    "    batches = []\n",
    "    \n",
    "    if shuffle:\n",
    "        permutation = list(np.random.permutation(m))\n",
    "        shuffled_X = X[:,permutation]\n",
    "        shuffled_Y = Y[:,permutation]\n",
    "    \n",
    "    else:\n",
    "        shuffled_X = X\n",
    "        shuffled_Y = Y\n",
    "    \n",
    "    for i in range(m//batch_size):\n",
    "        mini_x = shuffled_X[:,i*batch_size:(i+1)*batch_size]\n",
    "        mini_y = shuffled_Y[:,i*batch_size:(i+1)*batch_size]\n",
    "        batch = (mini_x,mini_y)\n",
    "        batches.append(batch)\n",
    "    \n",
    "    if m%batch_size !=0:\n",
    "        mini_x = shuffled_X[:,batch_size*(m//batch_size):]\n",
    "        mini_y = shuffled_Y[:,batch_size*(m//batch_size):]\n",
    "        batch = (mini_x,mini_y)\n",
    "        batches.append(batch)\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_adam(parameters):\n",
    "    L = len(parameters)//2\n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        v[\"dW\"+str(l+1)] = np.zeros_like(parameters[\"W\"+str(l+1)])\n",
    "        v[\"db\"+str(l+1)] = np.zeros_like(parameters[\"b\"+str(l+1)])\n",
    "        \n",
    "        s[\"dW\"+str(l+1)] = np.zeros_like(parameters[\"W\"+str(l+1)])\n",
    "        s[\"db\"+str(l+1)] = np.zeros_like(parameters[\"b\"+str(l+1)])\n",
    "        \n",
    "    return v,s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_adam(parameters,grads,v,s,t,lr=0.01,beta1=0.9,beta2=0.999,epsilon=1e-7):\n",
    "    L = len(parameters)//2\n",
    "    v_corrected = {}\n",
    "    s_corrected = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l + 1)] = beta1 * v[\"dW\" + str(l + 1)] + (1 - beta1) * grads['dW' + str(l + 1)]\n",
    "        v[\"db\" + str(l + 1)] = beta1 * v[\"db\" + str(l + 1)] + (1 - beta1) * grads['db' + str(l + 1)]\n",
    "        \n",
    "        s[\"dW\" + str(l + 1)] = beta2 * s[\"dW\" + str(l + 1)] + (1 - beta2) * np.power(grads['dW' + str(l + 1)], 2)\n",
    "        s[\"db\" + str(l + 1)] = beta2 * s[\"db\" + str(l + 1)] + (1 - beta2) * np.power(grads['db' + str(l + 1)], 2)\n",
    "        \n",
    "        v_corrected[\"dW\" + str(l + 1)] = v[\"dW\" + str(l + 1)] / (1 - np.power(beta1, t))\n",
    "        v_corrected[\"db\" + str(l + 1)] = v[\"db\" + str(l + 1)] / (1 - np.power(beta1, t))\n",
    "        \n",
    "        s_corrected[\"dW\" + str(l + 1)] = s[\"dW\" + str(l + 1)] / (1 - np.power(beta2, t))\n",
    "        s_corrected[\"db\" + str(l + 1)] = s[\"db\" + str(l + 1)] / (1 - np.power(beta2, t))\n",
    "        \n",
    "        parameters[\"W\"+str(l+1)] = parameters[\"W\"+str(l+1)] - lr * (v_corrected[\"dW\"+str(l+1)]/np.sqrt(s_corrected[\"dW\"+str(l+1)]+epsilon))\n",
    "        parameters[\"b\"+str(l+1)] = parameters[\"b\"+str(l+1)] - lr * (v_corrected[\"db\"+str(l+1)]/np.sqrt(s_corrected[\"db\"+str(l+1)]+epsilon))\n",
    "        \n",
    "    return parameters,v,s  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,Y,parameters):\n",
    "    Y_pred,c = forward_propagation(X,parameters)\n",
    "    prediction = np.argmax(Y_pred, axis=0)\n",
    "    labels = np.argmax(Y, axis=0)\n",
    "\n",
    "    diff=prediction-labels \n",
    "    accuracy = 1.0 - (float(np.count_nonzero(diff)) / len(diff))\n",
    "    print(accuracy*100)\n",
    "    return accuracy*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X,Y,layer_dims,parameters=None,activations=None,drop_probs=None,lr=0.01,beta1=0.9,beta2=0.999,lambd=0,epochs=3,batch_size=256,shuffle=True,optimizer=\"adam\",print_cost=0):\n",
    "    costs = []\n",
    "    m = X.shape[1]\n",
    "    t=0\n",
    "#     if not parameters:\n",
    "#         try:\n",
    "#             parameters\n",
    "#         except NameError:\n",
    "#             parameters = initialize_parameters(layer_dims)\n",
    "    if parameters is None:\n",
    "        parameters = initialize_parameters(layer_dims)\n",
    "    \n",
    "    acts = organize_activations(activations,len(layer_dims)-1)\n",
    "    \n",
    "    if optimizer==\"adam\":\n",
    "        v,s = initialize_adam(parameters)\n",
    "        \n",
    "    keep_probs = get_keep_probs(drop_probs)\n",
    "        \n",
    "    for i in range(epochs):\n",
    "        cost = 0\n",
    "        mini_batches = random_mini_batches(X,Y,batch_size,shuffle)\n",
    "        for batch in mini_batches:\n",
    "            (mini_x,mini_y) = batch\n",
    "            if keep_probs is None:\n",
    "                A_L,caches = forward_propagation(mini_x,parameters,acts)\n",
    "            else:\n",
    "                A_L,caches = forward_propagation_with_dropouts(mini_x,parameters,acts=acts,keep_probs=keep_probs)\n",
    "            cost += loss_with_regularization(A_L,mini_y,parameters=parameters,lambd=lambd) #default lambd=0. \n",
    "            grads = backprop(A_L,mini_y,caches,acts=acts,lambd=lambd,keep_probs=keep_probs)\n",
    "            #update parameters:\n",
    "            if optimizer==\"adam\":\n",
    "                t = t+1\n",
    "                parameters,v,s = update_parameters_with_adam(parameters,grads,v,s,t,lr,beta1,beta2)\n",
    "            else:\n",
    "                parameters = update_parameters(parameters,grads,lr)\n",
    "        \n",
    "        cost = cost/len(mini_batches)\n",
    "        if print_cost!=0 and i % print_cost == 0:\n",
    "            print (\"Cost after iteration %i: %f\" % (i, cost))\n",
    "        if print_cost!=0 and i % print_cost == 0:\n",
    "            costs.append(cost)\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## README FOR FIT FUNCTION\n",
    "### X : x_train of shape (n_x,m)\n",
    "### Y : y_train of shape (n_classes,m) , hot_coding\n",
    "### layer_dims : full architechture : input, hidden,output in a tuple/list\n",
    "\n",
    "### activations: list of strings of activations. by default all relus and last sigmoid\n",
    "### parameters: if we have already trained parameters. else initialize them with function initialize_parameters()\n",
    "\n",
    "### drop_probs: if you wanna add dropout: list of dropouts for every layer (output layer is optional. takes 0(wont drop))\n",
    "### lr: learning rate. default 0.01\n",
    "### beta1: default 0.9\n",
    "### beta2: default 0.999\n",
    "### lambd: if regularization. default=0 i.e no regularization\n",
    "### epochs: default 3\n",
    "### batch_size: default 256\n",
    "### optimizer: default adam\n",
    "### print_cost: after how many iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [\"relu\",'relu',\"sigmoid\"]\n",
    "lay = [x_train.shape[0],256,128,10]\n",
    "drops = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.196730\n",
      "Cost after iteration 1: 0.155054\n",
      "Cost after iteration 2: 0.123349\n"
     ]
    }
   ],
   "source": [
    "p = fit(x_train,y_train,layer_dims=lay,activations=a,parameters=p,lr=0.001,epochs=3,print_cost=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "97.04"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(x_test,y_test,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{5, 7, 9}"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    hexa\n",
    "except NameError:\n",
    "    hexa = 7\n",
    "hexa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOUElEQVR4nO3dX4xUdZrG8ecF8R+DCkuHtAyRGTQmHY1AStgEg+hk8U+iwI2BGERjxAuQmQTiolzAhRdGd2YyihnTqAE2IxPCSITErIMEY4iJoVC2BZVFTeNA+FOE6Dh6gTLvXvRh0mLXr5qqU3XKfr+fpNPV56nT502Fh1Ndp7t+5u4CMPQNK3oAAK1B2YEgKDsQBGUHgqDsQBAXtfJgY8eO9YkTJ7bykEAovb29OnXqlA2UNVR2M7tT0h8kDZf0krs/nbr/xIkTVS6XGzkkgIRSqVQ1q/tpvJkNl/SCpLskdUlaYGZd9X4/AM3VyM/s0yR96u6fu/sZSX+WNCefsQDkrZGyj5f0t35fH8m2/YCZLTazspmVK5VKA4cD0Iimvxrv7t3uXnL3UkdHR7MPB6CKRsp+VNKEfl//PNsGoA01UvY9kq4zs1+Y2cWS5kvals9YAPJW96U3d//ezJZKelN9l95ecfcDuU0GIFcNXWd39zckvZHTLACaiF+XBYKg7EAQlB0IgrIDQVB2IAjKDgRB2YEgKDsQBGUHgqDsQBCUHQiCsgNBUHYgCMoOBEHZgSAoOxAEZQeCoOxAEJQdCIKyA0FQdiAIyg4EQdmBICg7EARlB4Kg7EAQlB0IgrIDQVB2IIiGVnFF+zt79mwy/+qrr5p6/LVr11bNvv322+S+Bw8eTOYvvPBCMl+xYkXVbNOmTcl9L7300mS+cuXKZL569epkXoSGym5mvZK+lnRW0vfuXspjKAD5y+PMfpu7n8rh+wBoIn5mB4JotOwu6a9mttfMFg90BzNbbGZlMytXKpUGDwegXo2W/RZ3nyrpLklLzGzm+Xdw9253L7l7qaOjo8HDAahXQ2V396PZ55OStkqalsdQAPJXd9nNbKSZjTp3W9JsSfvzGgxAvhp5NX6cpK1mdu77vOru/5PLVEPMF198kczPnDmTzN99991kvnv37qrZl19+mdx3y5YtybxIEyZMSOaPPfZYMt+6dWvVbNSoUcl9b7rppmR+6623JvN2VHfZ3f1zSelHBEDb4NIbEARlB4Kg7EAQlB0IgrIDQfAnrjn44IMPkvntt9+ezJv9Z6btavjw4cn8qaeeSuYjR45M5vfff3/V7Oqrr07uO3r06GR+/fXXJ/N2xJkdCIKyA0FQdiAIyg4EQdmBICg7EARlB4LgOnsOrrnmmmQ+duzYZN7O19mnT5+ezGtdj961a1fV7OKLL07uu3DhwmSOC8OZHQiCsgNBUHYgCMoOBEHZgSAoOxAEZQeC4Dp7DsaMGZPMn3322WS+ffv2ZD5lypRkvmzZsmSeMnny5GT+1ltvJfNaf1O+f3/1pQSee+655L7IF2d2IAjKDgRB2YEgKDsQBGUHgqDsQBCUHQiC6+wtMHfu3GRe633lay0v3NPTUzV76aWXkvuuWLEimde6jl7LDTfcUDXr7u5u6HvjwtQ8s5vZK2Z20sz299s2xsx2mNmh7HP6HQwAFG4wT+PXS7rzvG0rJe109+sk7cy+BtDGapbd3d+RdPq8zXMkbchub5A0N9+xAOSt3hfoxrn7sez2cUnjqt3RzBabWdnMypVKpc7DAWhUw6/Gu7tL8kTe7e4ldy91dHQ0ejgAdaq37CfMrFOSss8n8xsJQDPUW/ZtkhZltxdJej2fcQA0S83r7Ga2SdIsSWPN7Iik1ZKelrTZzB6WdFjSfc0ccqi74oorGtr/yiuvrHvfWtfh58+fn8yHDeP3sn4qapbd3RdUiX6V8ywAmoj/loEgKDsQBGUHgqDsQBCUHQiCP3EdAtasWVM127t3b3Lft99+O5nXeivp2bNnJ3O0D87sQBCUHQiCsgNBUHYgCMoOBEHZgSAoOxAE19mHgNTbPa9bty6579SpU5P5I488ksxvu+22ZF4qlapmS5YsSe5rZskcF4YzOxAEZQeCoOxAEJQdCIKyA0FQdiAIyg4EwXX2IW7SpEnJfP369cn8oYceSuYbN26sO//mm2+S+z7wwAPJvLOzM5njhzizA0FQdiAIyg4EQdmBICg7EARlB4Kg7EAQXGcPbt68ecn82muvTebLly9P5qn3nX/iiSeS+x4+fDiZr1q1KpmPHz8+mUdT88xuZq+Y2Ukz299v2xozO2pm+7KPu5s7JoBGDeZp/HpJdw6w/ffuPjn7eCPfsQDkrWbZ3f0dSadbMAuAJmrkBbqlZtaTPc0fXe1OZrbYzMpmVq5UKg0cDkAj6i37HyVNkjRZ0jFJv612R3fvdveSu5c6OjrqPByARtVVdnc/4e5n3f2fktZJmpbvWADyVlfZzaz/3xbOk7S/2n0BtIea19nNbJOkWZLGmtkRSaslzTKzyZJcUq+kR5s3Iop04403JvPNmzcn8+3bt1fNHnzwweS+L774YjI/dOhQMt+xY0cyj6Zm2d19wQCbX27CLACaiF+XBYKg7EAQlB0IgrIDQVB2IAhz95YdrFQqeblcbtnx0N4uueSSZP7dd98l8xEjRiTzN998s2o2a9as5L4/VaVSSeVyecC1rjmzA0FQdiAIyg4EQdmBICg7EARlB4Kg7EAQvJU0knp6epL5li1bkvmePXuqZrWuo9fS1dWVzGfOnNnQ9x9qOLMDQVB2IAjKDgRB2YEgKDsQBGUHgqDsQBBcZx/iDh48mMyff/75ZP7aa68l8+PHj1/wTIN10UXpf56dnZ3JfNgwzmX98WgAQVB2IAjKDgRB2YEgKDsQBGUHgqDsQBBcZ/8JqHUt+9VXX62arV27Nrlvb29vPSPl4uabb07mq1atSub33ntvnuMMeTXP7GY2wcx2mdlHZnbAzH6dbR9jZjvM7FD2eXTzxwVQr8E8jf9e0nJ375L075KWmFmXpJWSdrr7dZJ2Zl8DaFM1y+7ux9z9/ez215I+ljRe0hxJG7K7bZA0t0kzAsjBBb1AZ2YTJU2R9J6kce5+LIuOSxpXZZ/FZlY2s3KlUmlkVgANGHTZzexnkv4i6Tfu/vf+mfetDjngCpHu3u3uJXcvdXR0NDQsgPoNquxmNkJ9Rf+Tu5/7M6gTZtaZ5Z2STjZnRAB5qHnpzcxM0suSPnb33/WLtklaJOnp7PPrTZlwCDhx4kQyP3DgQDJfunRpMv/kk08ueKa8TJ8+PZk//vjjVbM5c+Yk9+VPVPM1mOvsMyQtlPShme3Ltj2pvpJvNrOHJR2WdF9TJgSQi5pld/fdkgZc3F3Sr/IdB0Cz8DwJCIKyA0FQdiAIyg4EQdmBIPgT10E6ffp01ezRRx9N7rtv375k/tlnn9UzUi5mzJiRzJcvX57M77jjjmR+2WWXXfBMaA7O7EAQlB0IgrIDQVB2IAjKDgRB2YEgKDsQRJjr7O+9914yf+aZZ5L5nj17qmZHjhypa6a8XH755VWzZcuWJfet9XbNI0eOrGsmtB/O7EAQlB0IgrIDQVB2IAjKDgRB2YEgKDsQRJjr7Fu3bm0ob0RXV1cyv+eee5L58OHDk/mKFSuqZldddVVyX8TBmR0IgrIDQVB2IAjKDgRB2YEgKDsQBGUHgjB3T9/BbIKkjZLGSXJJ3e7+BzNbI+kRSZXsrk+6+xup71UqlbxcLjc8NICBlUollcvlAVddHswv1Xwvabm7v29moyTtNbMdWfZ7d/+vvAYF0DyDWZ/9mKRj2e2vzexjSeObPRiAfF3Qz+xmNlHSFEnn3uNpqZn1mNkrZja6yj6LzaxsZuVKpTLQXQC0wKDLbmY/k/QXSb9x979L+qOkSZImq+/M/9uB9nP3bncvuXupo6Oj8YkB1GVQZTezEeor+p/c/TVJcvcT7n7W3f8paZ2kac0bE0CjapbdzEzSy5I+dvff9dve2e9u8yTtz388AHkZzKvxMyQtlPShme3Ltj0paYGZTVbf5bheSel1iwEUajCvxu+WNNB1u+Q1dQDthd+gA4Kg7EAQlB0IgrIDQVB2IAjKDgRB2YEgKDsQBGUHgqDsQBCUHQiCsgNBUHYgCMoOBFHzraRzPZhZRdLhfpvGSjrVsgEuTLvO1q5zScxWrzxnu8bdB3z/t5aW/UcHNyu7e6mwARLadbZ2nUtitnq1ajaexgNBUHYgiKLL3l3w8VPadbZ2nUtitnq1ZLZCf2YH0DpFn9kBtAhlB4IopOxmdqeZHTSzT81sZREzVGNmvWb2oZntM7NC15fO1tA7aWb7+20bY2Y7zOxQ9nnANfYKmm2NmR3NHrt9ZnZ3QbNNMLNdZvaRmR0ws19n2wt97BJzteRxa/nP7GY2XNL/SfoPSUck7ZG0wN0/aukgVZhZr6SSuxf+CxhmNlPSPyRtdPcbsm3PSDrt7k9n/1GOdvf/bJPZ1kj6R9HLeGerFXX2X2Zc0lxJD6rAxy4x131qweNWxJl9mqRP3f1zdz8j6c+S5hQwR9tz93cknT5v8xxJG7LbG9T3j6XlqszWFtz9mLu/n93+WtK5ZcYLfewSc7VEEWUfL+lv/b4+ovZa790l/dXM9prZ4qKHGcA4dz+W3T4uaVyRwwyg5jLerXTeMuNt89jVs/x5o3iB7sducfepku6StCR7utqWvO9nsHa6djqoZbxbZYBlxv+lyMeu3uXPG1VE2Y9KmtDv659n29qCux/NPp+UtFXttxT1iXMr6GafTxY8z7+00zLeAy0zrjZ47Ipc/ryIsu+RdJ2Z/cLMLpY0X9K2Aub4ETMbmb1wIjMbKWm22m8p6m2SFmW3F0l6vcBZfqBdlvGutsy4Cn7sCl/+3N1b/iHpbvW9Iv+ZpFVFzFBlrl9K+t/s40DRs0napL6ndd+p77WNhyX9m6Sdkg5JekvSmDaa7b8lfSipR33F6ixotlvU9xS9R9K+7OPuoh+7xFwtedz4dVkgCF6gA4Kg7EAQlB0IgrIDQVB2IAjKDgRB2YEg/h/vpjt5hXz6+gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mnist_data = tf.keras.datasets.mnist\n",
    "(x, y), (x_test, y_test) = mnist_data.load_data()\n",
    "# x_train=np.reshape(x_train,(60000,784))\n",
    "# x_train = x_train.T\n",
    "# y_new = np.eye(10)[y_train.astype('int32')]\n",
    "# print(y_train.shape)\n",
    "# y_new = y_new.T.reshape(10,60000)\n",
    "# y_train = y_new\n",
    "# print(x_train.shape)\n",
    "# print(y_new[:,0])\n",
    "\n",
    "x=np.reshape(x,(60000,784))\n",
    "x = x.T\n",
    "i = np.arange(y.shape[0])\n",
    "y_ = np.zeros((10,60000))\n",
    "y_[y.astype(int),i] = 1\n",
    "plt.imshow(x[:,0].reshape(28,28), cmap = plt.cm.binary)\n",
    "\n",
    "x = x/255.0\n",
    "#separating train and test\n",
    "x_train = x[:,:-10000]\n",
    "x_test = x[:,-10000:]\n",
    "y_test = y_[:,-10000:]\n",
    "y_train = y_[:,:-10000]\n",
    "# x_train = tf.keras.utils.normalize(x_train, axis=1)\n",
    "# x_test = tf.keras.utils.normalize(x_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=(28,28)))\n",
    "model.add(tf.keras.layers.Dense(units=1024, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "model.add(tf.keras.layers.Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1024)              803840    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 814,090\n",
      "Trainable params: 814,090\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================]50000/50000 [==============================] - 41s 816us/step - loss: 0.1239 - acc: 0.9630 - val_loss: 0.0949 - val_acc: 0.9736\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x7f0207fed2b0>"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer= 'adam' , loss ='categorical_crossentropy' , metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=1,validation_data=(x_test,y_test),batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.T\n",
    "x_test = x_test.T\n",
    "y_test = y_test.T\n",
    "y_train = y_train.T\n",
    "x_train = x_train.reshape((50000,28,28))\n",
    "x_test = x_test.reshape((10000,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
